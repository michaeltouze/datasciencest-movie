{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-101e72c4e46e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "if sc:\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-75a40fce4652>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mJVM\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \"\"\"\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_launch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36m_launch_gateway\u001b[1;34m(conf, insecure)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext().getOrCreate()\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_raw_imdb = \"data/raw_data/imdb.dataset/\"\n",
    "path_parsed_imdb = \"data/parsed_data/imdb.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## name.basics\n",
    "### name.basic\n",
    "#### PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_basics = spark.read.csv(\n",
    "    os.path.join(path_raw_imdb, \"name.basics.tsv\"), \n",
    "    sep=\"\\t\", \n",
    "    header=True, \n",
    "    nullValue=\"\\\\N\", \n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_basics.write.parquet(os.path.join(path_parsed_imdb, \"name_basics\"), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nconst: string (nullable = true)\n",
      " |-- primaryName: string (nullable = true)\n",
      " |-- birthYear: integer (nullable = true)\n",
      " |-- deathYear: integer (nullable = true)\n",
      " |-- primaryProfession: string (nullable = true)\n",
      " |-- knownForTitles: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name_basics = spark.read.parquet(os.path.join(path_parsed_imdb, \"name_basics\"))\n",
    "name_basics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+---------+---------+--------------------+--------------------+\n",
      "|   nconst|     primaryName|birthYear|deathYear|   primaryProfession|      knownForTitles|\n",
      "+---------+----------------+---------+---------+--------------------+--------------------+\n",
      "|nm1380630|Giorgos Kyriakos|     null|     null|producer,producti...|tt5392738,tt03571...|\n",
      "|nm1380631|           Kysse|     null|     null|             actress|           tt0289752|\n",
      "|nm1380632| Kämy Kämäräinen|     null|     null|               actor| tt1802456,tt0168123|\n",
      "|nm1380633| Calle Kärnekull|     null|     null|              stunts|           tt0098796|\n",
      "|nm1380634|    Imre Kõszegi|     1944|     null|    music_department|           tt0152796|\n",
      "+---------+----------------+---------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name_basics.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_basics = pd.read_csv(\n",
    "    os.path.join(path_raw, \"name.basics.tsv\"), \n",
    "    sep=\"\\t\", \n",
    "    na_values=\"\\\\N\", \n",
    "    index_col=0, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_basics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_basics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_basics.to_parquet(os.path.join(path_parsed_imdb, \"name_basics.parquet\"), engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### link.name.basics.titles\n",
    "#### PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_basics = spark.read.parquet(os.path.join(path_parsed_imdb, \"name_basics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+---------+---------+--------------------+--------------------+\n",
      "|   nconst|     primaryName|birthYear|deathYear|   primaryProfession|      knownForTitles|\n",
      "+---------+----------------+---------+---------+--------------------+--------------------+\n",
      "|nm1380630|Giorgos Kyriakos|     null|     null|producer,producti...|tt5392738,tt03571...|\n",
      "|nm1380631|           Kysse|     null|     null|             actress|           tt0289752|\n",
      "|nm1380632| Kämy Kämäräinen|     null|     null|               actor| tt1802456,tt0168123|\n",
      "|nm1380633| Calle Kärnekull|     null|     null|              stunts|           tt0098796|\n",
      "|nm1380634|    Imre Kõszegi|     1944|     null|    music_department|           tt0152796|\n",
      "+---------+----------------+---------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name_basics.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_name_basics_titles = (\n",
    "    name_basics\n",
    "    .select(\"nconst\", F.explode(F.split(name_basics.knownForTitles, \",\")).alias(\"tconst\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|   nconst|   tconst|\n",
      "+---------+---------+\n",
      "|nm1380630|tt5392738|\n",
      "|nm1380630|tt0357123|\n",
      "|nm1380630|tt1111843|\n",
      "|nm1380630|tt6304968|\n",
      "|nm1380631|tt0289752|\n",
      "+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "link_name_basics_titles.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_name_basics_titles.write.parquet(os.path.join(path_parsed_imdb, \"link_name_basics_titles\"), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## title.basics\n",
    "##### PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_basics = spark.read.csv(\n",
    "    os.path.join(path_raw_imdb, \"title.basics.tsv\"), \n",
    "    sep=\"\\t\", \n",
    "    header=True, \n",
    "    nullValue=\"\\\\N\", \n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_basics.write.parquet( os.path.join(path_parsed_imdb, \"title_basics\"), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- titleType: string (nullable = true)\n",
      " |-- primaryTitle: string (nullable = true)\n",
      " |-- originalTitle: string (nullable = true)\n",
      " |-- isAdult: integer (nullable = true)\n",
      " |-- startYear: integer (nullable = true)\n",
      " |-- endYear: integer (nullable = true)\n",
      " |-- runtimeMinutes: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title_basics = spark.read.parquet( os.path.join(path_parsed_imdb, \"title_basics\"))\n",
    "title_basics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|              genres|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|tt0000001|    short|          Carmencita|          Carmencita|      0|     1894|   null|             1|   Documentary,Short|\n",
      "|tt0000002|    short|Le clown et ses c...|Le clown et ses c...|      0|     1892|   null|             5|     Animation,Short|\n",
      "|tt0000003|    short|      Pauvre Pierrot|      Pauvre Pierrot|      0|     1892|   null|             4|Animation,Comedy,...|\n",
      "|tt0000004|    short|         Un bon bock|         Un bon bock|      0|     1892|   null|          null|     Animation,Short|\n",
      "|tt0000005|    short|    Blacksmith Scene|    Blacksmith Scene|      0|     1893|   null|             1|        Comedy,Short|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title_basics.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_basics = pd.read_csv(\n",
    "    os.path.join(path_raw, \"title.basics.tsv\"), \n",
    "    sep=\"\\t\", \n",
    "    na_values=\"\\\\N\", \n",
    "    index_col=0, \n",
    "    dtype={\"runtimeMinutes\": \"object\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_basics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_basics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_basics.runtimeMinutes = title_basics.runtimeMinutes.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title_basics.to_parquet(os.path.join(path_parsed_imdb, \"title_basics.parquet\"), engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## title.ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_ratings = spark.read.csv(\n",
    "    os.path.join(path_raw_imdb, \"title.ratings.tsv\"), \n",
    "    sep=\"\\t\", \n",
    "    header=True, \n",
    "    nullValue=\"\\\\N\", \n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_ratings.write.parquet(os.path.join(path_parsed_imdb, \"title_ratings\"), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- averageRating: double (nullable = true)\n",
      " |-- numVotes: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title_ratings = spark.read.parquet(os.path.join(path_parsed_imdb, \"title_ratings\"))\n",
    "title_ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+\n",
      "|   tconst|averageRating|numVotes|\n",
      "+---------+-------------+--------+\n",
      "|tt0000001|          5.6|    1531|\n",
      "|tt0000002|          6.1|     185|\n",
      "|tt0000003|          6.5|    1179|\n",
      "|tt0000004|          6.2|     113|\n",
      "|tt0000005|          6.1|    1896|\n",
      "+---------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title_ratings.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_ratings = pd.read_csv(\n",
    "    os.path.join(path_raw, \"title.ratings.tsv\"), \n",
    "    sep=\"\\t\", \n",
    "    na_values=\"\\\\N\", \n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_ratings.to_parquet(os.path.join(path_parsed_imdb, \"title_ratings.parquet\"), engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## title.principals\n",
    "#### PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_principals = spark.read.csv(\n",
    "    os.path.join(path_raw_imdb, \"title.principals.tsv\"), \n",
    "    sep=\"\\t\", \n",
    "    header=True, \n",
    "    nullValue=\"\\\\N\", \n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_principals.write.parquet(os.path.join(path_parsed_imdb, \"title_principals\"), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- ordering: integer (nullable = true)\n",
      " |-- nconst: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- characters: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title_principals = spark.read.parquet(os.path.join(path_parsed_imdb, \"title_principals\"))\n",
    "title_principals.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+---------------+--------------------+-----------+\n",
      "|   tconst|ordering|   nconst|       category|                 job| characters|\n",
      "+---------+--------+---------+---------------+--------------------+-----------+\n",
      "|tt0000001|       1|nm1588970|           self|                null|[\"Herself\"]|\n",
      "|tt0000001|       2|nm0005690|       director|                null|       null|\n",
      "|tt0000001|       3|nm0374658|cinematographer|director of photo...|       null|\n",
      "|tt0000002|       1|nm0721526|       director|                null|       null|\n",
      "|tt0000002|       2|nm1335271|       composer|                null|       null|\n",
      "+---------+--------+---------+---------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title_principals.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539\n",
      "(35314063, 5)\n",
      "Index(['tt0000001', 'tt0000001', 'tt0000001', 'tt0000002', 'tt0000002',\n",
      "       'tt0000003', 'tt0000003', 'tt0000003', 'tt0000003', 'tt0000004',\n",
      "       ...\n",
      "       'tt9916880', 'tt9916880', 'tt9916880', 'tt9916880', 'tt9916880',\n",
      "       'tt9916880', 'tt9916880', 'tt9916880', 'tt9916880', 'tt9916880'],\n",
      "      dtype='object', name='tconst', length=35314063)\n"
     ]
    }
   ],
   "source": [
    "path_file = os.path.join(path_raw_imdb, \"title.principals.tsv\")\n",
    "chunk_size = 2 ** 16\n",
    "d_type = {\n",
    "    \"tconst\": \"object\",\n",
    "    \"ordering\": \"int64\",\n",
    "    \"nconst\": \"object\",\n",
    "    \"category\": \"object\",\n",
    "    \"job\": \"object\",\n",
    "    \"characters\": \"object\"\n",
    "}\n",
    "\n",
    "title_principals = pd.DataFrame()\n",
    "_iter = 0\n",
    "for chunk in pd.read_csv(path_file, sep=\"\\t\", na_values=\"\\\\N\", index_col=0, dtype=d_type, chunksize=chunk_size):\n",
    "    title_principals = pd.concat([title_principals, chunk])\n",
    "    _iter += 1\n",
    "print(_iter)\n",
    "print(title_principals.shape)\n",
    "print(title_principals.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ordering</th>\n",
       "      <th>nconst</th>\n",
       "      <th>category</th>\n",
       "      <th>job</th>\n",
       "      <th>characters</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tconst</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tt0000001</th>\n",
       "      <td>1</td>\n",
       "      <td>nm1588970</td>\n",
       "      <td>self</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"Herself\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt0000001</th>\n",
       "      <td>2</td>\n",
       "      <td>nm0005690</td>\n",
       "      <td>director</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt0000001</th>\n",
       "      <td>3</td>\n",
       "      <td>nm0374658</td>\n",
       "      <td>cinematographer</td>\n",
       "      <td>director of photography</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt0000002</th>\n",
       "      <td>1</td>\n",
       "      <td>nm0721526</td>\n",
       "      <td>director</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt0000002</th>\n",
       "      <td>2</td>\n",
       "      <td>nm1335271</td>\n",
       "      <td>composer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ordering     nconst         category                      job  \\\n",
       "tconst                                                                     \n",
       "tt0000001         1  nm1588970             self                      NaN   \n",
       "tt0000001         2  nm0005690         director                      NaN   \n",
       "tt0000001         3  nm0374658  cinematographer  director of photography   \n",
       "tt0000002         1  nm0721526         director                      NaN   \n",
       "tt0000002         2  nm1335271         composer                      NaN   \n",
       "\n",
       "            characters  \n",
       "tconst                  \n",
       "tt0000001  [\"Herself\"]  \n",
       "tt0000001          NaN  \n",
       "tt0000001          NaN  \n",
       "tt0000002          NaN  \n",
       "tt0000002          NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_principals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 35314063 entries, tt0000001 to tt9916880\n",
      "Data columns (total 5 columns):\n",
      "ordering      int64\n",
      "nconst        object\n",
      "category      object\n",
      "job           object\n",
      "characters    object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.6+ GB\n"
     ]
    }
   ],
   "source": [
    "title_principals.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title_principals.to_parquet(os.path.join(path_parsed_imdb, \"title_principals.parquet\"), engine=\"pyarrow\")\n",
    "#title_principals.write.parquet(os.path.join(path_parsed_imdb, \"title_principals.parquet\"), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## title.crew\n",
    "### title.crew\n",
    "#### PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_crew = spark.read.csv(\n",
    "    os.path.join(path_raw_imdb, \"title.crew.tsv\"), \n",
    "    sep=\"\\t\", \n",
    "    header=True, \n",
    "    nullValue=\"\\\\N\", \n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_crew.write.parquet(os.path.join(path_parsed_imdb, \"title_crew\"), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- directors: string (nullable = true)\n",
      " |-- writers: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title_crew = spark.read.parquet(os.path.join(path_parsed_imdb, \"title_crew\"))\n",
    "title_crew.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------------------+\n",
      "|   tconst|          directors|             writers|\n",
      "+---------+-------------------+--------------------+\n",
      "|tt3093900|               null|                null|\n",
      "|tt3093902|               null|                null|\n",
      "|tt3093904|               null|                null|\n",
      "|tt3093906|nm3983984,nm3985542|nm5844710,nm39839...|\n",
      "|tt3093908|          nm1787707|           nm1787707|\n",
      "+---------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title_crew.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_crew = pd.read_csv(\n",
    "    os.path.join(path_raw, \"title.crew.tsv\"), \n",
    "    sep=\"\\t\", \n",
    "    na_values=\"\\\\N\", \n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_crew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_crew.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-88107d24d2cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtitle_crew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_parsed_imdb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"title_crew\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pyarrow\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m             raise AttributeError(\n\u001b[1;32m-> 1182\u001b[1;33m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[0;32m   1183\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_parquet'"
     ]
    }
   ],
   "source": [
    "title_crew.to_parquet(os.path.join(path_parsed_imdb, \"title_crew\"), engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### link.title.directors\n",
    "#### PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_crew = spark.read.parquet(os.path.join(path_parsed_imdb, \"title_crew\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------------------+\n",
      "|   tconst|          directors|             writers|\n",
      "+---------+-------------------+--------------------+\n",
      "|tt3093900|               null|                null|\n",
      "|tt3093902|               null|                null|\n",
      "|tt3093904|               null|                null|\n",
      "|tt3093906|nm3983984,nm3985542|nm5844710,nm39839...|\n",
      "|tt3093908|          nm1787707|           nm1787707|\n",
      "+---------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title_crew.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_title_directors = (\n",
    "    title_crew.select(\"tconst\", F.explode(F.split(title_crew.directors, \",\")).alias(\"nconsts\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|   tconst|  nconsts|\n",
      "+---------+---------+\n",
      "|tt3093906|nm3983984|\n",
      "|tt3093906|nm3985542|\n",
      "|tt3093908|nm1787707|\n",
      "|tt3093916|nm0943944|\n",
      "|tt3093918|nm0943944|\n",
      "+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "link_title_directors.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_title_directors.write.parquet(os.path.join(path_parsed_imdb, \"link_title_directors\"), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## title.akas\n",
    "### title.akas\n",
    "#### PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_akas = spark.read.csv(\n",
    "    os.path.join(path_raw_imdb, \"title.akas.tsv\"), \n",
    "    sep=\"\\t\", \n",
    "    header=True, \n",
    "    nullValue=\"\\\\N\", \n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_akas.write.parquet(os.path.join(path_parsed_imdb, \"title_akas\"), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- titleId: string (nullable = true)\n",
      " |-- ordering: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- types: string (nullable = true)\n",
      " |-- attributes: string (nullable = true)\n",
      " |-- isOriginalTitle: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title_akas = spark.read.parquet(os.path.join(path_parsed_imdb, \"title_akas\"))\n",
    "title_akas.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+------+--------+-----------+----------+---------------+\n",
      "|  titleId|ordering|     title|region|language|      types|attributes|isOriginalTitle|\n",
      "+---------+--------+----------+------+--------+-----------+----------+---------------+\n",
      "|tt1172049|       5|Разрушение|    BG|      bg|       null|      null|              0|\n",
      "|tt1172049|       6| Demolição|    PT|    null|imdbDisplay|      null|              0|\n",
      "|tt1172049|       7|   Rušenje|    HR|    null|       null|      null|              0|\n",
      "|tt1172049|       8|Darabokban|    HU|    null|imdbDisplay|      null|              0|\n",
      "|tt1172049|       9|Destrukcja|    PL|    null|imdbDisplay|      null|              0|\n",
      "+---------+--------+----------+------+--------+-----------+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title_akas.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_akas = pd.read_csv(\n",
    "    os.path.join(path_raw, \"title.akas.tsv\"), \n",
    "    sep=\"\\t\", \n",
    "    na_values=\"\\\\N\", \n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_akas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_akas.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_akas.to_parquet(os.path.join(path_parsed_imdb, \"title_akas.parquet\"), engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
